{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f63463",
   "metadata": {},
   "source": [
    "\n",
    "# StockAlert · Fast Prompting POC\n",
    "\n",
    "**Objetivo:** Mostrar cómo técnicas de *Fast Prompting* mejoran una tarea de sugerir pedidos y alertas de stock en un lote de SKUs, reduciendo costo (menos llamadas) y aumentando consistencia (JSON válido, reglas claras).\n",
    "\n",
    "> La notebook corre 100% offline con un **Mock LLM**.\n",
    "> Si querés integrar un proveedor real, hay una celda para habilitarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os, json, math, hashlib, textwrap, random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pathlib import Path\n",
    "DATA = Path(\"../data\")\n",
    "SRC = Path(\"../src\")\n",
    "\n",
    "# Visual settings (no custom colors per project rules)\n",
    "plt.rcParams['figure.figsize'] = (8, 4)\n",
    "\n",
    "print(\"Ready. Python version OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "products = pd.read_csv(DATA / \"products.csv\")\n",
    "sales = pd.read_csv(DATA / \"sales.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Compute a simple 7-day avg sales (synthetic)\n",
    "recent = sales[sales['date'] >= sales['date'].max() - pd.Timedelta(days=7)]\n",
    "avg_sales = recent.groupby('product_id')['units_sold'].mean().rename('avg_7d')\n",
    "df = products.merge(avg_sales, on='product_id', how='left').fillna({'avg_7d': 0})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383eff9",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt variants\n",
    "\n",
    "- **Baseline**: prompt largo sin estructura rígida.\n",
    "- **Fast-01**: rol+tarea + **JSON schema** + campos obligatorios + manejo de faltantes.\n",
    "- **Fast-02**: agrega **few-shot** y reglas explícitas, manteniendo salida estricta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load prompt templates\n",
    "import importlib.util, sys\n",
    "spec = importlib.util.spec_from_file_location(\"pipeline\", SRC / \"pipeline.py\")\n",
    "pipeline = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"pipeline\"] = pipeline\n",
    "spec.loader.exec_module(pipeline)\n",
    "\n",
    "baseline = pipeline.baseline_template()\n",
    "fast01 = pipeline.fast01_template()\n",
    "fast02 = pipeline.fast02_template()\n",
    "\n",
    "baseline, fast01, fast02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c8170",
   "metadata": {},
   "source": [
    "\n",
    "## Mock LLM (offline)\n",
    "\n",
    "Para esta POC, usamos un **simulador** que aplica reglas heurísticas y **finge** la salida del LLM en **JSON**.\n",
    "Esto nos permite medir:\n",
    "- **Validez de formateo** (siempre válido en el mock, pero contabilizamos como proxy).\n",
    "- **Acierto** de *REORDER vs HOLD* comparado con una heurística de referencia.\n",
    "- **Costo estimado**: nº de llamadas simuladas y longitud de prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_heuristic(row):\n",
    "    # Reference rule for evaluation (ground truth proxy)\n",
    "    # Reorder if stock below reorder_point; if avg_7d very low and shelf life short, prefer HOLD\n",
    "    if row['current_stock'] < row['reorder_point']:\n",
    "        risk_expiry = 'LOW'\n",
    "        if row['shelf_life_days'] <= 20 and row['avg_7d'] < 10:\n",
    "            risk_expiry = 'HIGH'\n",
    "        return {\n",
    "            \"product_id\": row['product_id'],\n",
    "            \"action\": \"REORDER\" if risk_expiry != 'HIGH' else \"HOLD\",\n",
    "            \"qty\": int(row['reorder_qty'] if risk_expiry != 'HIGH' else 0),\n",
    "            \"reasons\": [\"stock_bajo\"] if risk_expiry != 'HIGH' else [\"riesgo_vencimiento\"],\n",
    "            \"risk_expiry\": risk_expiry\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"product_id\": row['product_id'],\n",
    "            \"action\": \"HOLD\",\n",
    "            \"qty\": 0,\n",
    "            \"reasons\": [\"stock_suficiente\"],\n",
    "            \"risk_expiry\": \"LOW\"\n",
    "        }\n",
    "\n",
    "def mock_llm(prompt_variant, payload_products):\n",
    "    # Produce a JSON compatible list using a similar (but not identical) rule to create noise\n",
    "    results = []\n",
    "    for row in payload_products.to_dict(orient='records'):\n",
    "        # Small variation vs heuristic to simulate \"model\" differences\n",
    "        base = simple_heuristic(pd.Series(row))\n",
    "        # tweak: sometimes suggest smaller qty when close to reorder_point\n",
    "        if base[\"action\"] == \"REORDER\" and row['current_stock'] >= (row['reorder_point'] * 0.9):\n",
    "            base[\"qty\"] = max(0, base[\"qty\"] - int(row['reorder_qty']*0.2))\n",
    "            base[\"reasons\"].append(\"ajuste_fino\")\n",
    "        results.append(base)\n",
    "    return {\"results\": results}\n",
    "\n",
    "def run_batch(prompt_variant, batch_df, max_skus_per_call=6):\n",
    "    # Consolidate multiple SKUs in one \"call\" to reduce cost\n",
    "    calls = []\n",
    "    results = []\n",
    "    for i in range(0, len(batch_df), max_skus_per_call):\n",
    "        sub = batch_df.iloc[i:i+max_skus_per_call]\n",
    "        payload = {\"products\": sub.to_dict(orient='records')}\n",
    "        # Simulate token size as the length of the JSON payload + template header size\n",
    "        prompt_len = len(json.dumps(payload)) + len(prompt_variant.template)\n",
    "        response = mock_llm(prompt_variant, sub)\n",
    "        calls.append({\"prompt_len\": prompt_len, \"n_skus\": len(sub)})\n",
    "        results.extend(response[\"results\"])\n",
    "    return results, calls\n",
    "\n",
    "def evaluate(results, reference_df):\n",
    "    # Compare REORDER vs HOLD decision against heuristic\n",
    "    ref = []\n",
    "    for _, row in reference_df.iterrows():\n",
    "        gt = simple_heuristic(row)  # ground truth proxy\n",
    "        ref.append((row['product_id'], gt['action']))\n",
    "    ref_map = dict(ref)\n",
    "    acc = 0\n",
    "    for r in results:\n",
    "        if ref_map.get(r['product_id']) == r['action']:\n",
    "            acc += 1\n",
    "    return acc / len(reference_df)\n",
    "\n",
    "df_eval = df.copy()\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b63e2",
   "metadata": {},
   "source": [
    "\n",
    "## Experimentos\n",
    "\n",
    "Comparamos **Baseline**, **Fast-01** y **Fast-02**, con *batching* de 6 SKUs por llamada.\n",
    "Métricas:\n",
    "- **Accuracy** vs. heurística (proxy).\n",
    "- **Prompts por SKU** (costo): nº de llamadas y tamaño promedio del prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variants = [baseline, fast01, fast02]\n",
    "\n",
    "records = []\n",
    "for v in variants:\n",
    "    res, calls = run_batch(v, df_eval, max_skus_per_call=6)\n",
    "    acc = evaluate(res, df_eval)\n",
    "    total_calls = len(calls)\n",
    "    avg_prompt_len = sum(c['prompt_len'] for c in calls)/max(1, total_calls)\n",
    "    records.append({\n",
    "        \"variant\": v.name,\n",
    "        \"accuracy_proxy\": round(acc, 3),\n",
    "        \"n_calls\": total_calls,\n",
    "        \"avg_prompt_len_chars\": int(avg_prompt_len)\n",
    "    })\n",
    "\n",
    "exp = pd.DataFrame(records)\n",
    "exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple bar chart for the accuracy proxy\n",
    "plt.figure()\n",
    "plt.bar(exp['variant'], exp['accuracy_proxy'])\n",
    "plt.title(\"Accuracy proxy por variante\")\n",
    "plt.xlabel(\"Variante\")\n",
    "plt.ylabel(\"Accuracy proxy (0-1)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3abf8",
   "metadata": {},
   "source": [
    "\n",
    "## Resultados y análisis\n",
    "\n",
    "- **Fast-01** y **Fast-02** utilizan **salida JSON estricta** → evita rondas extra para corregir formato.\n",
    "- **Few-shot** (Fast-02) y reglas explícitas mejoran la consistencia con las heurísticas.\n",
    "- **Costo**: con *batching* (6 SKUs / llamada) reducimos a 1 llamada para el dataset ejemplo.\n",
    "\n",
    "> En un entorno real, además mediríamos tokens (prompt+completion) y latencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f132c",
   "metadata": {},
   "source": [
    "\n",
    "## (Opcional) Integración con proveedor LLM\n",
    "\n",
    "Descomenta y completa con tu API key para probar en producción. Mantén el JSON schema y el batching para controlar costos.\n",
    "\n",
    "**Nota:** Esta celda está comentada para que la notebook sea 100% ejecutable sin internet ni credenciales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# import os, json\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "#\n",
    "# def real_llm_call(prompt_text: str) -> dict:\n",
    "#     # Ajusta el modelo según tu acceso (ej: 'gpt-4o-mini' o similar)\n",
    "#     resp = client.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[\n",
    "#             {\"role\":\"system\", \"content\":\"Eres un analista de inventario experto. Responde SOLO con JSON válido.\"},\n",
    "#             {\"role\":\"user\", \"content\": prompt_text},\n",
    "#         ],\n",
    "#         temperature=0.1,\n",
    "#         max_tokens=600\n",
    "#     )\n",
    "#     txt = resp.choices[0].message.content\n",
    "#     return json.loads(txt)\n",
    "#\n",
    "# # Para usar: reemplazar mock_llm(...) por real_llm_call(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fcf44e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusión (vs. Preentrega 1)\n",
    "\n",
    "- Pasamos de una idea **descriptiva/marketing** a una **POC reproducible**.\n",
    "- Las técnicas de **Fast Prompting** (schema, few-shot, constraints, batching) mejoran **calidad y costos**.\n",
    "- El siguiente paso es integrar el LLM real y conectar con tu fuente de datos/ERP para pruebas con productos reales.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
